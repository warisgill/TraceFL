parallel_processes: 0 # parallelizing provenance
client_weights_normalization: False # provenance
single_prov_key: For-Main-Training--google-bert/bert-base-cased-yahoo_answers_topics-faulty_clients[[]]-noise_rateNone-TClients100-fedavg-(R25-clientsPerR10)-non_iid_dirichlet0.1-batch32-epochs2-lr0.001

check_train_cache: True # if set to True then if the config is already in the cache then it will not retrain.
check_prov_cache: True
check_dataset_cache: True


dataset_channels:
    cifar10: 3 # RGB
    femnist: 3 # Grey
    mnist: 3
    pathmnist: 3
    organamnist: 3
    superb: None
    dbpedia_14: None
    yahoo_answers_topics: None

dataset_classes:
    cifar10: 10
    femnist: 10
    mnist: 10
    pathmnist: 9
    organamnist: 11
    superb: 12 # audio
    yahoo_answers_topics: 10 # Text
    dbpedia_14: 14 # Text

model_arch:
    microsoft/deberta-v3-base: transformer
    squeezebert/squeezebert-uncased: transformer
    distilbert/distilbert-base-uncased: transformer
    microsoft/MiniLM-L12-H384-uncased: transformer
    openai-community/openai-gpt: transformer
    google-bert/bert-base-cased: transformer
    google-bert/bert-large-cased: transformer
    # meta-llama/Llama-2-7b-hf: transformer
    # FacebookAI/roberta-base: transformer
    Intel/dynamic_tinybert: transformer
    facebook/wav2vec2-base: transformer
    openai/whisper-tiny: transformer
    resnet18: cnn
    densenet121: cnn

storage:
    dir: storage_of_exps/
    train_cache_name: cache_of_training
    results_cache_name: cache_of_approach
    fl_datasets_cache: cache_of_fl_datasets
    fed_debug_cache_name: cache_of_fed_debug_results

# fed_debug config
multirun_faulty_clients: -1 # use for hydra multirun to pass clients ids to `faulty_clients_ids` as we cannot pass list of  list in multirun
noise_rate: None
faulty_clients_ids: [] # malacious client(s) ids in a list
label2flip: None
neuron_activation_threshold: 0.003
faster_input_generation: True

